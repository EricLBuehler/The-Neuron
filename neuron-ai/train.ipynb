{"cells":[{"cell_type":"markdown","source":["##**Classifier model based on PerceiverIO**\n","##Main driver for The Neuron\n","Eric Buehler 2022"],"metadata":{"id":"mSxqXm-43wb3"},"id":"mSxqXm-43wb3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0774bc81"},"outputs":[],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","prefix='/content/drive/MyDrive/Colab Notebooks/neuron'\n","prefix_='/content/drive/MyDrive/\"Colab Notebooks\"/neuron'\n","modelname=\"4_24_22_m2\"\n","\n","prefix_models=prefix+\"/models/\"+modelname+\"/\"\n","\n","if not os.path.exists(prefix_models):\n","    os.makedirs(prefix_models)\n","os.chdir(prefix)"],"id":"0774bc81"},{"cell_type":"code","execution_count":null,"metadata":{"id":"151fda3b"},"outputs":[],"source":["import os,sys\n","import math\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import cv2\n","import PIL \n","from PIL import Image, ImageOps\n","\n","import pickle\n","import tqdm\n"],"id":"151fda3b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dcc4758"},"outputs":[],"source":["!nvidia-smi -L"],"id":"2dcc4758"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c05d970f"},"outputs":[],"source":["!nvidia-smi "],"id":"c05d970f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4f9af61c"},"outputs":[],"source":["import torch\n"," \n","import torchvision\n"," \n","import matplotlib.pyplot as plt\n","import numpy as np\n"," \n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ExponentialLR, StepLR, LambdaLR\n","\n","print(\"Torch version:\", torch.__version__) "],"id":"4f9af61c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTRhHZGGKtZC"},"outputs":[],"source":["!git clone https://github.com/lucidrains/perceiver-pytorch\n","!cd perceiver-pytorch/\n","!pip install perceiver-pytorch"],"id":"DTRhHZGGKtZC"},{"cell_type":"code","execution_count":null,"metadata":{"id":"08af6d65"},"outputs":[],"source":["import torch\n","from perceiver_pytorch import Perceiver\n","from perceiver_pytorch import PerceiverIO\n"," \n","from torch.utils.data import DataLoader,Dataset\n","from torchvision.io import read_image\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","from PIL import Image\n","\n","to_pil = transforms.ToPILImage()"],"id":"08af6d65"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a199f9c5"},"outputs":[],"source":["from torch.autograd import Variable"],"id":"a199f9c5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7439325"},"outputs":[],"source":["numchannel=3\n","batchSize=8\n","CPUonly=False"],"id":"f7439325"},{"cell_type":"code","execution_count":null,"metadata":{"id":"63357d1f"},"outputs":[],"source":["class ImageDataset(Dataset):\n","    def __init__(self,dir,mode,images,transform):\n","        self.images=list(images)\n","        self.mode=list(mode[\"Mode\"])\n","        self.speed=list(mode[\"Speed\"])\n","        self.transform=transform\n","        self.dir=dir+\"/images/image_\"\n","        \n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self,index):\n","        im_pil = Image.open(self.dir+str(int(self.images[index]))+\".png\").convert('RGB')\n","\n","        image=self.transform(im_pil)\n","\n","        return (self.mode[index],image)"],"id":"63357d1f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fE05RaQvb_pM"},"outputs":[],"source":["\"\"\"\n","Left=1\n","Right=2\n","Fwd=3\n","Bwd=4\n","Stop=0\n","\"\"\""],"id":"fE05RaQvb_pM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a890381f"},"outputs":[],"source":["import torchvision.transforms as T\n","\n","data_dir = './data_v4'\n","\n","def load_split_train_test(data_dir,csvfile, valid_size = .2):\n","    train_transforms = transforms.Compose([\n","                                     transforms.Resize((80,160)),\n","                                       transforms.ToTensor()\n","                                       ])\n","    test_transforms = transforms.Compose([\n","                                         transforms.Resize((80,160)),\n","                                       transforms.ToTensor()\n","                                       ])\n","\n","    df=pd.read_csv(csvfile)\n","    mode=df.iloc[:,1:]\n","    images=df.iloc[:,0]\n","    X_train, X_test, y_train, y_test =train_test_split(mode,images,test_size=valid_size)\n","    train_data=ImageDataset(data_dir,X_train,y_train,train_transforms)\n","    test_data=ImageDataset(data_dir,X_test,y_test,test_transforms)\n","    trainloader = torch.utils.data.DataLoader(train_data, batch_size=batchSize)\n","\n","    testloader = torch.utils.data.DataLoader(test_data, batch_size=1)\n","    return trainloader, testloader\n","\n","csvfile=data_dir+\"/train.csv\"\n","dataloader, testloader = load_split_train_test(data_dir,csvfile, .5)\n","print(\"Number of training batches: \", len(dataloader), \"batch size= \", batchSize, \"total: \",len(dataloader)*batchSize)\n","print(\"Number of test batches: \", len(testloader), \"batch size= \", 1, \"total: \",len(testloader))\n","\n","print(\"TOTAL images (account for full batches): \", len(dataloader)*batchSize+len(testloader) )"],"id":"a890381f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYFfAz5W4REV"},"outputs":[],"source":["print(len(dataloader))\n","print(len(dataloader)*batchSize)"],"id":"QYFfAz5W4REV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aESmdliOvrtt"},"outputs":[],"source":["input=next(iter(testloader))[1]\n","image = to_pil(input[0])\n","print(image.size)\n","plt.imshow(image)\n","plt.show()"],"id":"aESmdliOvrtt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtLrcBlKJ6zu"},"outputs":[],"source":["im_resx=image.size[0]\n","im_resy=image.size[1]"],"id":"EtLrcBlKJ6zu"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9784442f"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() \n","                                  else \"cpu\")\n","\n","if CPUonly == True:\n","     device = torch.device(\"cpu\")\n","     print(\"CPU\")"],"id":"9784442f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"XzHT5VHB2hS5"},"outputs":[],"source":["from torch.nn.modules.dropout import Dropout2d\n","\n","\n","class network(nn.Module):\n","    def __init__(self,nclasses):\n","        super(network, self).__init__()\n","        self.queries_dim_=256\n","        latent_dim_=784\n","        logits_dim= nclasses\n","        embed_dim=128\n","        embed_dim_min=16\n","        dim=3\n","        self.embed_dim_=embed_dim\n","\n","        self.convert_up = nn.Sequential( #numchannel -> embed_dim\n","            nn.Linear(  numchannel, 4),\n","            nn.Linear(  4, 8),\n","            nn.Linear(  8, 16),\n","            nn.Linear(  16, 32),\n","            nn.Linear(  32, 48),\n","            nn.Linear(  48, embed_dim)\n","        )\n","\n","        self.encoders=[]\n","        embed_dim_=embed_dim\n","        while embed_dim_>embed_dim_min:\n","            layer=nn.Conv2d(in_channels=embed_dim_, out_channels=int(embed_dim_/2), kernel_size=3, stride=1, padding=1).to(device)\n","            self.encoders.append(layer)\n","            embed_dim_=int(embed_dim_/2)\n","\n","        self.decoders=[]\n","        embed_dim_=embed_dim_min\n","        while embed_dim_<embed_dim:\n","            layer=nn.Conv2d(in_channels=embed_dim_, out_channels=int(embed_dim_*2), kernel_size=3, stride=1, padding=1).to(device)\n","            self.decoders.append(layer)\n","            embed_dim_=int(embed_dim_*2)\n","            \n","        self.convs=[]\n","        for item in self.encoders:\n","            self.convs.append(item)\n","        self.convs.append(nn.BatchNorm2d(embed_dim_min))\n","        for item in self.decoders:\n","            self.convs.append(item)\n","        self.convs.append(nn.BatchNorm2d(embed_dim))\n","        self.convs=nn.ModuleList(self.convs)\n","\n","        self.query_gen = nn.Sequential( # embed_dim*4 -> self.queries_dim_\n","            nn.Linear(  embed_dim*4, 384),\n","            nn.Linear(  384, 512),\n","            nn.Linear(  512, 640),\n","            nn.Linear(  640, 784),\n","            nn.Linear(  784, 1024),\n","            nn.Linear(  1024, 784),\n","            nn.Linear(  784, 640),\n","            nn.Linear(  640, 512),\n","            nn.Linear(  512, self.queries_dim_),\n","        )\n","        \n","        self.convert_down = nn.Sequential( #im_resx*im_resy -> 1\n","            nn.Linear(  im_resx*im_resy, 2048),\n","            nn.Linear(  2048, 1024),\n","            nn.Linear(  1024, 784),\n","            nn.Linear(  784, 512),\n","            nn.Linear(  512, 256),\n","            nn.Linear(  256, 128),\n","            nn.Linear(  128, 1),\n","        )\n","\n","        self.pos_emb_x = nn.Embedding(im_resy, embed_dim*1)\n","        self.pos_emb_y = nn.Embedding(im_resx, embed_dim*1)\n","\n","        self.pos_matrix_i = torch.zeros (im_resx, im_resy, dtype=torch.long)\n","        self.pos_matrix_j = torch.zeros (im_resx, im_resy,dtype=torch.long)\n","        for i in range(im_resy):\n","            for j in range(im_resx):\n","                self.pos_matrix_i [j,i]=i\n","                self.pos_matrix_j [j,i]=j\n","                       \n","        self.pos_matrix_j =torch.flatten(self.pos_matrix_j , start_dim=0, end_dim=1) \n","        self.pos_matrix_i =torch.flatten(self.pos_matrix_i , start_dim=0, end_dim=1)  \n","\n","        self.model = PerceiverIO(\n","            dim = embed_dim*4,\n","            queries_dim = self.queries_dim_,\n","            logits_dim = nclasses,\n","            depth = 12,\n","            num_latents = 512,\n","            latent_dim = latent_dim_,\n","            cross_heads = 1,\n","            latent_heads = 8,\n","            cross_dim_head = 64,\n","            latent_dim_head = 64,\n","            weight_tie_layers = False,\n","            decoder_ff=True\n","        ).to(device)\n","\n","        self.softmax=nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        x=torch.permute(x,(0,2,3,1))\n","        x=self.convert_up(x)\n","        x=torch.permute(x,(0,3,1,2))\n","        x_=x.clone() \n","        x=torch.flatten(x, start_dim=2, end_dim=3) \n","\n","        for layer in self.convs:\n","            x_=layer(x_)\n","        x_=torch.flatten(x_, start_dim=2, end_dim=3) \n","\n","        x_=torch.permute(x_, (0,2,1)  )\n","        x=torch.permute(x, (0,2,1)  )\n","\n","        x=torch.cat([x,x_],dim=2)\n","        \n","        pos_matrix_j_=self.pos_matrix_j.repeat(x.shape[0], 1, 1).to(device=device) \n","        pos_matrix_i_=self.pos_matrix_i.repeat(x.shape[0], 1, 1).to(device=device) \n","        \n","\n","        pos_emb_y = self.pos_emb_y(pos_matrix_j_)\n","        pos_emb_y = torch.squeeze(pos_emb_y, 1)\n","        pos_emb_x = self.pos_emb_x( pos_matrix_i_)\n","        pos_emb_x = torch.squeeze(pos_emb_x, 1)\n","\n","        catlist=[x,pos_emb_y,pos_emb_x]\n","\n","        inputs= torch.cat(catlist, 2)\n","        queries=self.query_gen(inputs)\n","        outputs=self.model(inputs,queries=queries )\n","\n","        outputs=torch.permute(outputs, (0,2,1)  )\n","        outputs=self.convert_down(outputs)\n","        outputs=torch.permute(outputs, (0,2,1)  )\n","        outputs=outputs.squeeze_()\n","        outputs=self.softmax(outputs)\n","        return outputs\n","        "],"id":"XzHT5VHB2hS5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoVlJp8pI0vo"},"outputs":[],"source":["print(im_resx*im_resy)"],"id":"xoVlJp8pI0vo"},{"cell_type":"code","execution_count":null,"metadata":{"id":"KR40BgOZZuBF"},"outputs":[],"source":["nclasses=5\n","startepoch=0\n","\n","model = network(nclasses)\n","params=model.to(device)\n","\n","#inputs=torch.randn(1,3,im_resy,im_resx).to(device)\n","#outputs=model(inputs)\n","#print(outputs.shape)"],"id":"KR40BgOZZuBF"},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2XFPj_5ihhj"},"outputs":[],"source":["from prettytable import PrettyTable\n","\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        params = parameter.numel()\n","        table.add_row([name, params])\n","    print(table)\n","\n","    print(f\"Total params: {sum(p.numel() for p in model.parameters())}  Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}  Untrainable params: {sum(p.numel() for p in model.parameters())-sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"],"id":"z2XFPj_5ihhj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXHWycfShu4A"},"outputs":[],"source":["count_parameters(model)"],"id":"sXHWycfShu4A"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlv3YUuzBg2n"},"outputs":[],"source":["!nvidia-smi"],"id":"mlv3YUuzBg2n"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9815bc1a"},"outputs":[],"source":["criterion =  nn.MSELoss()\n","\n","optimizer = optim.AdamW(model.parameters() , lr=0.000005)\n","\n","scheduler = StepLR(optimizer, gamma=0.9, step_size=2)\n"],"id":"9815bc1a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b1d9f1a"},"outputs":[],"source":["print(im_resx)\n","print(im_resy)"],"id":"0b1d9f1a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"79aa3c7b","scrolled":false},"outputs":[],"source":["epochs=190\n","train_losses, test_losses,val_acc = [], [], []\n","steps = 0\n","print_every  = len (dataloader)\n","running_loss = 0.0\n","\n","torch.cuda.empty_cache()\n","\n","for epoch in range(startepoch, epochs):\n","    train_losses_epoch,test_losses_epoch=[],[]\n","    correct_train,correct_test=0,0\n","    print(f\"Epoch {epoch+1}/{epochs}\")\n","    for mode, inputs in tqdm.tqdm(dataloader):\n","        optimizer.zero_grad()\n","        \n","        steps += 1\n","\n","        labels=mode.long()\n","        labels=F.one_hot(labels,num_classes=nclasses)\n","        \n","        inputs,labels = inputs.to(device),labels.to(device)\n","        outputs=model(inputs)\n","        loss = criterion(outputs.float(), labels.float() )\n","        \n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        \n","        outputs_=outputs.argmax(-1).cpu().detach()\n","        labels_=labels.argmax(-1).cpu().detach()\n","        correct_train += (outputs_ == labels_).float().sum()\n","\n","    accuracy_train = 100 * correct_train / (len(dataloader)*batchSize)\n","        \n","    test_loss = 0\n","    accuracy = 0\n","    model.eval()\n","    \n","    print(\"\\nNow evaluate test batches...\")\n","    with torch.no_grad():\n","        for mode, inputs in testloader:\n","            optimizer.zero_grad()\n","\n","            labels=mode.long()\n","            labels=F.one_hot(labels,num_classes=nclasses)\n","\n","            inputs,labels = inputs.to(device),labels.to(device)\n","            \n","            outputs=model(inputs)\n","            outputs = torch.unsqueeze(outputs, dim=0)\n","            outputs_=outputs.argmax(-1).cpu().detach()\n","            batch_loss = criterion(outputs.float(), labels.float() )\n","            test_loss += batch_loss.item()\n","                \n","            numr=batchSize-1\n","\n","            labels_=labels.argmax(-1).cpu().detach()\n","            correct_test += (outputs_ == labels_).float().sum()\n","\n","    accuracy_test = 100 * correct_test / len(testloader)\n","\n","    \n","\n","    train_losses.append(running_loss/print_every)\n","    train_losses_epoch.append(running_loss/print_every)\n","    test_losses.append(test_loss/len(testloader))    \n","    test_losses_epoch.append(test_loss/len(testloader))  \n","        \n","    print(f\"Epoch {epoch+1}/{epochs} \"\n","            f\"Train loss: {running_loss/print_every:.6f} \"\n","            f\"Test loss: {test_loss/len(testloader):.6f} \"\n","            f\"Train accuracy: {accuracy_train}% \"\n","            f\"Test accuracy: {accuracy_test}% \"\n","            )\n","    \n","    running_loss = 0\n","    model.train()\n","    \n","    with open(prefix_models+\"train_loss.txt\",\"a\") as file:\n","        for item in train_losses_epoch:\n","            file.write(f\"E{epoch}_{item}\\n\")\n","\n","    with open(prefix_models+\"test_loss.txt\",\"a\") as file:\n","        for item in test_losses_epoch:\n","            file.write(f\"E{epoch}_{item}\\n\")\n","\n","    with open(prefix_models+\"train_acc.txt\",\"a\") as file:\n","        file.write(f\"E{epoch}_{accuracy_train}\\n\")\n","\n","    with open(prefix_models+\"test_acc.txt\",\"a\") as file:\n","        file.write(f\"E{epoch}_{accuracy_test}\\n\")\n","\n","    scheduler.step()\n","\n","    fgg=f\"model_E{epoch}.pth\"\n","    namesve = prefix_models+fgg\n","    torch.save(model, namesve)\n","    \n","\n","print('Finished Training')"],"id":"79aa3c7b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"A49mMkMAc7BS"},"outputs":[],"source":["label,input=next(iter(testloader))\n","outputs=model(input.to(device))\n","print(\"OUTPUTS\",outputs)\n","outputs_=outputs.argmax(-1).cpu().detach()\n","print(\"PRED\",outputs_)\n","\n","image = to_pil(input[0])\n","print(image.size)\n","plt.imshow(image)\n","plt.show()\n","\n","print(\"REAL\",label)"],"id":"A49mMkMAc7BS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"SdgyUmfdxSOo"},"outputs":[],"source":["\n","fgg=f\"model_final.pth\"\n","namesve = prefix_models+fgg\n","torch.save(model,namesve) \n"],"id":"SdgyUmfdxSOo"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1lxmuHfd16zY"},"outputs":[],"source":["for mode, inputs in testloader:\n","    labels=mode.long()\n","    labels=F.one_hot(labels,num_classes=nclasses)\n","\n","    optimizer.zero_grad()\n","    \n","    inputs,labels = inputs.to(device),labels.to(device)\n","    outputs=model(inputs)\n","    print(outputs)\n","    \n","    loss = criterion(outputs.float(), labels.float() )\n","    pred_mode=outputs.argmax(-1).cpu().detach().numpy()\n","    real_mode=labels.argmax(-1).cpu().detach().numpy()\n","    print(\"Pred:\",pred_mode,\" Real:\",real_mode)"],"id":"1lxmuHfd16zY"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"train_v32_final.ipynb","provenance":[{"file_id":"1-I6SSVT53_cPJG8zuzgm8_BEm6VXvATI","timestamp":1642727311650}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}
